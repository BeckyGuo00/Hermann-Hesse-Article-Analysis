{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words in beneath_the_wheel.txt:\n",
      "hans:303\n",
      "could:139\n",
      "would:135\n",
      "time:134\n",
      "like:100\n",
      "felt:90\n",
      "good:86\n",
      "back:84\n",
      "father:82\n",
      "heilner:75\n",
      "boys:74\n",
      "even:73\n",
      "well:73\n",
      "away:72\n",
      "home:71\n",
      "Most common words in demian.txt:\n",
      "like:156\n",
      "world:150\n",
      "time:143\n",
      "demian:142\n",
      "could:135\n",
      "would:125\n",
      "said:123\n",
      "know:108\n",
      "felt:107\n",
      "something:103\n",
      "gutenberg:97\n",
      "mother:97\n",
      "much:90\n",
      "life:90\n",
      "project:88\n",
      "Most common words in gertrude.txt:\n",
      "would:174\n",
      "time:171\n",
      "could:146\n",
      "good:120\n",
      "muoth:119\n",
      "life:109\n",
      "like:108\n",
      "said:108\n",
      "thought:98\n",
      "music:96\n",
      "looked:94\n",
      "well:93\n",
      "know:93\n",
      "felt:90\n",
      "long:86\n",
      "Most common words in if_the_war_goes_on.txt:\n",
      "world:210\n",
      "people:127\n",
      "good:112\n",
      "time:106\n",
      "life:99\n",
      "every:84\n",
      "years:83\n",
      "great:83\n",
      "even:79\n",
      "must:79\n",
      "would:78\n",
      "many:77\n",
      "like:73\n",
      "friends:72\n",
      "long:71\n",
      "Most common words in in_sight_of_chaos.txt:\n",
      "every:46\n",
      "dostoevsky:28\n",
      "world:26\n",
      "must:23\n",
      "karamazoff:21\n",
      "europe:20\n",
      "thing:18\n",
      "time:17\n",
      "karamazoffs:17\n",
      "people:17\n",
      "downfall:16\n",
      "like:16\n",
      "good:16\n",
      "even:15\n",
      "european:14\n",
      "Most common words in knulp.txt:\n",
      "knulp:212\n",
      "said:111\n",
      "little:83\n",
      "good:74\n",
      "like:74\n",
      "know:66\n",
      "could:60\n",
      "tanner:59\n",
      "time:58\n",
      "back:57\n",
      "would:56\n",
      "looked:56\n",
      "still:51\n",
      "right:50\n",
      "went:50\n",
      "Most common words in narziss_and_goldmund.txt:\n",
      "goldmund:584\n",
      "would:487\n",
      "could:386\n",
      "narziss:291\n",
      "never:262\n",
      "little:245\n",
      "life:237\n",
      "love:231\n",
      "long:230\n",
      "eyes:227\n",
      "heart:209\n",
      "still:206\n",
      "many:200\n",
      "said:194\n",
      "though:192\n",
      "Most common words in peter_camenzind.txt:\n",
      "would:240\n",
      "life:141\n",
      "time:125\n",
      "like:110\n",
      "love:107\n",
      "even:97\n",
      "could:96\n",
      "little:93\n",
      "father:92\n",
      "felt:81\n",
      "people:79\n",
      "first:78\n",
      "beautiful:75\n",
      "made:73\n",
      "much:67\n",
      "Most common words in rosshalde.txt:\n",
      "pierre:253\n",
      "veraguth:208\n",
      "little:170\n",
      "said:168\n",
      "would:161\n",
      "like:138\n",
      "albert:124\n",
      "rosshalde:120\n",
      "looked:116\n",
      "child:116\n",
      "good:113\n",
      "time:109\n",
      "eyes:105\n",
      "hermann:94\n",
      "hesse:94\n",
      "Most common words in siddhartha.txt:\n",
      "siddhartha:408\n",
      "govinda:146\n",
      "time:139\n",
      "like:137\n",
      "would:121\n",
      "also:115\n",
      "river:110\n",
      "many:98\n",
      "gutenberg:97\n",
      "long:97\n",
      "said:95\n",
      "project:88\n",
      "life:88\n",
      "kamala:83\n",
      "love:82\n",
      "Most common words in steppenwolf.txt:\n",
      "life:225\n",
      "little:189\n",
      "time:167\n",
      "like:156\n",
      "would:142\n",
      "world:141\n",
      "could:133\n",
      "said:127\n",
      "good:123\n",
      "well:122\n",
      "even:118\n",
      "music:117\n",
      "hermine:114\n",
      "love:110\n",
      "know:106\n",
      "Most common words in the_glass_bead_game.txt:\n",
      "knecht:732\n",
      "game:664\n",
      "would:617\n",
      "time:553\n",
      "life:487\n",
      "could:432\n",
      "master:380\n",
      "world:360\n",
      "joseph:358\n",
      "music:336\n",
      "magister:332\n",
      "even:308\n",
      "glass:307\n",
      "order:307\n",
      "bead:299\n",
      "Most common words in the_journey_to_the_east.txt:\n",
      "journey:145\n",
      "east:123\n",
      "league:123\n",
      "time:56\n",
      "said:52\n",
      "also:50\n",
      "could:46\n",
      "like:45\n",
      "many:45\n",
      "seemed:42\n",
      "would:40\n",
      "still:38\n",
      "even:36\n",
      "went:35\n",
      "great:34\n"
     ]
    }
   ],
   "source": [
    "# Remove the stopwords and find out the top 15 words used most in each novel\n",
    "\n",
    "import re\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "stopwords = set()\n",
    "#add stopwords file\n",
    "with open(\"english_stopwords.txt\", encoding = \"utf8\") as stopword_file:\n",
    "    for line in stopword_file:\n",
    "        word = line.strip()\n",
    "        stopwords.add(word)\n",
    "# remove stopwords and gengerate words\n",
    "def count_words(filepath):\n",
    "    with open(filepath, 'r', encoding='utf8') as file:\n",
    "        text = file.read().lower()\n",
    "    words = re.findall(r'\\b\\w+\\b', text)\n",
    "    filtered_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords and len(word) > 3:\n",
    "            filtered_words.append(word)\n",
    "    word_counts = Counter(filtered_words)\n",
    "    return word_counts\n",
    "       \n",
    "for file in os.scandir(\"plain_text_hesse\"):    \n",
    "    \n",
    "    if file.is_file():\n",
    "        word_counts = count_words(file.path)\n",
    "        print(f\"Most common words in {file.name}:\")\n",
    "        for word, count in word_counts.most_common(15):\n",
    "            print(f\"{word}:{count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## Doc [beneath_the_wheel.txt] weights\n",
      "heilner 0.27122478631037056\n",
      "time 0.20255014569068294\n",
      "come 0.16703810716049827\n",
      "look 0.15388550029746692\n",
      "feel 0.15388550029746692\n",
      "hand 0.14974959079356884\n",
      "college 0.1336538995228618\n",
      "good 0.13021080794401046\n",
      "vicar 0.12697120454671873\n",
      "make 0.12363450451249478\n",
      "\n",
      "## Doc [demian.txt] weights\n",
      "know 0.24113417970768597\n",
      "time 0.20216299914886804\n",
      "come 0.19363805340162663\n",
      "look 0.19242020400916357\n",
      "world 0.18998450522423743\n",
      "feel 0.17537031251468072\n",
      "dream 0.1631918185900501\n",
      "gutenberg 0.15778785193486727\n",
      "think 0.1473597764880303\n",
      "make 0.14370622831064114\n",
      "\n",
      "## Doc [gertrude.txt] weights\n",
      "muoth 0.335540131670315\n",
      "time 0.23991771176578772\n",
      "look 0.20235483770144722\n",
      "know 0.18660266470672376\n",
      "gertrude 0.18130887638751825\n",
      "think 0.17690901978689397\n",
      "feel 0.1550983187172769\n",
      "good 0.15146320187234072\n",
      "come 0.13328761764765984\n",
      "life 0.13086420641770238\n",
      "\n",
      "## Doc [if_the_war_goes_on.txt] weights\n",
      "world 0.29249643999433605\n",
      "time 0.2089260285673829\n",
      "good 0.1880334257106446\n",
      "people 0.18524774532974617\n",
      "life 0.17549786399660164\n",
      "make 0.16296230228255867\n",
      "know 0.14764106018761725\n",
      "great 0.13928401904492194\n",
      "thing 0.1295341377117774\n",
      "friend 0.12267698170480443\n",
      "\n",
      "## Doc [in_sight_of_chaos.txt] weights\n",
      "karamazoff 0.45737761255019427\n",
      "dostoevsky 0.29581927880997905\n",
      "downfall 0.18006610355105762\n",
      "idiot 0.15755784060717543\n",
      "world 0.14971356066568803\n",
      "thing 0.12198882720907915\n",
      "soul 0.09980904044379203\n",
      "thought 0.09980904044379203\n",
      "myshkin 0.09800948840361307\n",
      "time 0.09426409375247025\n",
      "\n",
      "## Doc [knulp.txt] weights\n",
      "tanner 0.30434041669773804\n",
      "know 0.24042599379231538\n",
      "look 0.23813622242286478\n",
      "come 0.19463056640330295\n",
      "little 0.18547148092550045\n",
      "good 0.1717328527087967\n",
      "machold 0.16863651790702183\n",
      "think 0.16715330996989547\n",
      "take 0.15341468175319173\n",
      "time 0.1488351390142905\n",
      "\n",
      "## Doc [narziss_and_goldmund.txt] weights\n",
      "goldmund 0.598039830920227\n",
      "come 0.1733577666028567\n",
      "know 0.15773994979178854\n",
      "cloister 0.15743451379348733\n",
      "love 0.15357519864217037\n",
      "life 0.13275144289407947\n",
      "little 0.12546312838224766\n",
      "make 0.11765421997671356\n",
      "heart 0.11401006272079765\n",
      "feel 0.10672174820896585\n",
      "\n",
      "## Doc [peter_camenzind.txt] weights\n",
      "time 0.22262878687794838\n",
      "life 0.22116412380638292\n",
      "love 0.19626485158977028\n",
      "make 0.18308288394568123\n",
      "feel 0.16257760094376494\n",
      "boppi 0.15533156906896275\n",
      "take 0.146466307156545\n",
      "father 0.14316177163357224\n",
      "know 0.13328433951245594\n",
      "little 0.13328433951245594\n",
      "\n",
      "## Doc [rosshalde.txt] weights\n",
      "pierre 0.577600314727356\n",
      "rosshalde 0.2262345140543387\n",
      "veraguth 0.19924054447745784\n",
      "look 0.17311417818704722\n",
      "little 0.1549419716370257\n",
      "come 0.14442122047648692\n",
      "burkhardt 0.14087812554325754\n",
      "studio 0.13358609401303806\n",
      "take 0.13007474162120675\n",
      "time 0.12911830969752142\n",
      "\n",
      "## Doc [siddhartha.txt] weights\n",
      "siddhartha 0.5438385783810457\n",
      "govinda 0.24911859593249797\n",
      "samana 0.22108080019621776\n",
      "brahman 0.1830259072157128\n",
      "time 0.16310146885522844\n",
      "gutenberg 0.12964335094446322\n",
      "know 0.12807968106422846\n",
      "river 0.1270356063043527\n",
      "teaching 0.12118934563303185\n",
      "learn 0.12107532350602847\n",
      "\n",
      "## Doc [steppenwolf.txt] weights\n",
      "life 0.22845124938163772\n",
      "steppenwolf 0.21238993424987032\n",
      "time 0.2012315260510596\n",
      "come 0.1963708611705992\n",
      "know 0.1924823292662309\n",
      "little 0.18081673355312602\n",
      "make 0.17692820164875772\n",
      "look 0.16915113784002112\n",
      "take 0.16331833998346865\n",
      "world 0.13901501558116677\n",
      "\n",
      "## Doc [the_glass_bead_game.txt] weights\n",
      "game 0.2629054714261413\n",
      "magister 0.24357259394880001\n",
      "castalia 0.22569752632797455\n",
      "time 0.22254500079940753\n",
      "life 0.17976064148029638\n",
      "knecht 0.1641436555112542\n",
      "bead 0.15569671196517143\n",
      "master 0.14570235476055798\n",
      "elite 0.1322832264619138\n",
      "take 0.12271482905481487\n",
      "\n",
      "## Doc [the_journey_to_the_east.txt] weights\n",
      "league 0.5302179058616578\n",
      "east 0.36941411473967967\n",
      "journey 0.3608239178826444\n",
      "time 0.17344627181079533\n",
      "seem 0.12633740786218425\n",
      "know 0.11134822387853527\n",
      "thing 0.10064166389021457\n",
      "archive 0.09995911340014861\n",
      "make 0.09421772789722216\n",
      "document 0.08692096817404228\n"
     ]
    }
   ],
   "source": [
    "#Use TF-IDF find the most weight of important for each novel\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# remove stopwords and gengerate words\n",
    "stopwords = set()\n",
    "#add stopwords file\n",
    "with open(\"english_stopwords.txt\", encoding = \"utf8\") as stopword_file:\n",
    "    for line in stopword_file:\n",
    "        word = line.strip()\n",
    "        stopwords.add(word)\n",
    "        \n",
    "def preprocess_text(filepath):\n",
    "    with open(filepath, 'r', encoding='utf8') as file:\n",
    "        text = file.read().lower()\n",
    "    try:\n",
    "        doc = nlp(text)\n",
    "    except ValueError:\n",
    "        nlp.max_length = len(text) + 1\n",
    "        doc = nlp(text)\n",
    "        \n",
    "    filtered_words = []    \n",
    "    for token in doc:\n",
    "        if (token.lemma_ not in stopwords and token.ent_type_ == \"\" and token.pos_ in (\"NOUN\", \"VERB\", \"ADJ\") and len(token.lemma_) > 3):\n",
    "            filtered_words.append(token.lemma_)\n",
    "    return \" \".join(filtered_words)  # list of text\n",
    "\n",
    "\n",
    "corpus = []\n",
    "doc_names = []\n",
    " \n",
    "for file in os.scandir(\"plain_text_hesse\"):\n",
    "    if file.is_file() and file.name.endswith(\".txt\"):\n",
    "        doc_names.append(file.name)\n",
    "        preprocessed = preprocess_text(file.path)\n",
    "        corpus.append(preprocessed)\n",
    "\n",
    "        \n",
    "\n",
    "# use TF-IDF vectorization\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorized_corpus = vectorizer.fit_transform(corpus)\n",
    "document_term_matrix = pd.DataFrame(vectorized_corpus.toarray(), index=doc_names, columns=vectorizer.get_feature_names_out())\n",
    "# print the top 5 weighted for each doc\n",
    "for doc_name, doc_row in document_term_matrix.iterrows():\n",
    "    print(f\"Doc [{doc_name}] weights\")\n",
    "    # sort terms and get top 5\n",
    "    for word, weight in doc_row.sort_values(ascending=False).head(10).items():\n",
    "        print(word, weight)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF trigram matrix finish'\n",
      "Document similarities:\n",
      "Document: beneath_the_wheel.txt\n",
      "knulp.txt:0.0005\n",
      "the_glass_bead_game.txt:0.0004\n",
      "demian.txt:0.0004\n",
      "Document: demian.txt\n",
      "siddhartha.txt:0.0905\n",
      "rosshalde.txt:0.0008\n",
      "steppenwolf.txt:0.0008\n",
      "Document: gertrude.txt\n",
      "knulp.txt:0.0009\n",
      "demian.txt:0.0005\n",
      "peter_camenzind.txt:0.0005\n",
      "Document: if_the_war_goes_on.txt\n",
      "the_glass_bead_game.txt:0.0031\n",
      "siddhartha.txt:0.0006\n",
      "knulp.txt:0.0004\n",
      "Document: in_sight_of_chaos.txt\n",
      "if_the_war_goes_on.txt:0.0002\n",
      "peter_camenzind.txt:0.0002\n",
      "demian.txt:0.0002\n",
      "Document: knulp.txt\n",
      "gertrude.txt:0.0009\n",
      "rosshalde.txt:0.0008\n",
      "beneath_the_wheel.txt:0.0005\n",
      "Document: narziss_and_goldmund.txt\n",
      "the_glass_bead_game.txt:0.0079\n",
      "steppenwolf.txt:0.0005\n",
      "rosshalde.txt:0.0005\n",
      "Document: peter_camenzind.txt\n",
      "the_glass_bead_game.txt:0.0031\n",
      "steppenwolf.txt:0.0005\n",
      "gertrude.txt:0.0005\n",
      "Document: rosshalde.txt\n",
      "the_glass_bead_game.txt:0.0038\n",
      "steppenwolf.txt:0.0009\n",
      "siddhartha.txt:0.0008\n",
      "Document: siddhartha.txt\n",
      "demian.txt:0.0905\n",
      "the_journey_to_the_east.txt:0.0010\n",
      "rosshalde.txt:0.0008\n",
      "Document: steppenwolf.txt\n",
      "rosshalde.txt:0.0009\n",
      "demian.txt:0.0008\n",
      "the_glass_bead_game.txt:0.0007\n",
      "Document: the_glass_bead_game.txt\n",
      "narziss_and_goldmund.txt:0.0079\n",
      "rosshalde.txt:0.0038\n",
      "if_the_war_goes_on.txt:0.0031\n",
      "Document: the_journey_to_the_east.txt\n",
      "siddhartha.txt:0.0010\n",
      "rosshalde.txt:0.0004\n",
      "gertrude.txt:0.0004\n"
     ]
    }
   ],
   "source": [
    "# Find out the Top similarity novel by using cosine_similarity\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "# remove stopwords and gengerate words\n",
    "stopwords = set()\n",
    "#add stopwords file\n",
    "with open(\"english_stopwords.txt\", encoding = \"utf8\") as stopword_file:\n",
    "    for line in stopword_file:\n",
    "        word = line.strip()\n",
    "        stopwords.add(word)\n",
    "        \n",
    "def preprocess_text(filepath):\n",
    "    with open(filepath, 'r', encoding='utf8') as file:\n",
    "        text = file.read().lower()\n",
    "    try:\n",
    "        doc = nlp(text)\n",
    "    except ValueError:\n",
    "        nlp.max_length = len(text) + 1\n",
    "        doc = nlp(text)\n",
    "        \n",
    "    filtered_words = []    \n",
    "    for token in doc:\n",
    "        if (token.lemma_ not in stopwords and token.ent_type_ == \"\" and token.pos_ in (\"NOUN\", \"VERB\", \"ADJ\") and len(token.lemma_) > 3):\n",
    "            filtered_words.append(token.lemma_)\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "filenames = []\n",
    "documents = []\n",
    "for file in os.scandir(\"plain_text_hesse\"):\n",
    "    if file.is_file() and file.name.endswith(\".txt\"):\n",
    "        filenames.append(file.name)\n",
    "        preprocessed = preprocess_text(file.path)\n",
    "        documents.append(preprocessed)\n",
    "        \n",
    "        \n",
    "# get trigrams with TF-IDF weighting       \n",
    "vectorizer = TfidfVectorizer(ngram_range=(3,3))\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), index=filenames, columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "print(\"TF-IDF trigram matrix finish'\")\n",
    "\n",
    "\n",
    "# cosine similarity measures similarity between document.\n",
    "cosine_sim = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "# convert the cosine similarity matrix\n",
    "cosine_sim_df = pd.DataFrame(cosine_sim, index=filenames, columns=filenames)\n",
    "\n",
    "\n",
    "#print Top similar documents for each\n",
    "print(\"Document similarities:\")\n",
    "for i, filename in enumerate(filenames):\n",
    "    # Top 3 similar documents\n",
    "    similar_docs = cosine_sim_df.iloc[i].sort_values(ascending=False)[1:4]  \n",
    "    print(f\"Document: {filename}\")\n",
    "    for doc, score in similar_docs.items():\n",
    "        print(f\"{doc}:{score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_20031",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
