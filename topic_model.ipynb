{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add text to metadata csv file as text column\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "csv_file = \"metadata of hesse project.csv\"\n",
    "df = pd.read_csv(csv_file)\n",
    "text_folder = \"plain_text_hesse\"\n",
    "\n",
    "text_data = {}\n",
    "for file in os.listdir(text_folder):\n",
    "    if file.endswith(\".txt\"):  # Ensure it's a text file\n",
    "        file_path = os.path.join(text_folder, file)\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            text_data[file] = f.read().strip()  # Store filename as key and content as value\n",
    "\n",
    "# Add a new column to the dataframe by matching the filename\n",
    "df[\"text\"] = df[\"filename\"].map(text_data).astype(str)\n",
    "df[\"text\"] = df[\"text\"].replace(\"nan\", \"\")\n",
    "\n",
    "# Save the updated CSV file\n",
    "df.to_csv(\"hesse_file.csv\", index=False)\n",
    "print(\"Merge complete! Updated file saved as 'hesse_file.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete! Filtered text saved in 'hesse_file_noun.csv'.\n"
     ]
    }
   ],
   "source": [
    "# creat a new csv file contain the metadata and only NOUN words \n",
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def extract_nouns(text):\n",
    "    filtered_tokens = []\n",
    "    try:\n",
    "        doc = nlp(text)  # Process text with spaCy\n",
    "    except ValueError:\n",
    "        nlp.max_length = len(text) + 1  # Dynamically adjust max_length\n",
    "        doc = nlp(text)\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'NOUN':\n",
    "            filtered_tokens.append(token.lemma_)\n",
    "    return \" \".join(filtered_tokens)\n",
    "\n",
    "df = pd.read_csv(\"hesse_file.csv\")\n",
    "\n",
    "df['filtered_text'] = df['text'].apply(extract_nouns)\n",
    "df.to_csv('hesse_file_noun.csv', index =False) \n",
    "\n",
    "print(\"Processing complete! Filtered text saved in 'hesse_file_noun.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing the text data done\n",
      "Fitting the NMF model done\n",
      "Topic 0: student,pupil,conversation,subject,position,week,passion,dignity,history,circle,stage,condition,atmosphere,instruction,harmony\n",
      "Topic 1: cloister,city,scholar,abbot,monk,horse,journeyman,workshop,knight,maid,forest,today,penance,priest,mund\n",
      "Topic 2: teaching,ferryman,forest,goal,merchant,grove,monk,boat,donation,city,dear,ferry,offering,oneness,trademark\n",
      "Topic 3: karamazoff,morality,downfall,culture,instinct,epileptic,mankind,ment,primeval,hysteria,ture,unconscious,dostoevsky,sult,pronouncement\n",
      "Topic 4: east,servant,ment,goal,tion,leader,official,faith,bench,violin,accuser,judgment,document,archive,hall\n",
      "Topic 5: elite,player,meditation,monastery,assignment,culture,pupil,festival,today,scholar,hierarchy,herdsman,magister,tutor,petition\n",
      "Topic 6: vicar,fishing,cider,examination,headache,shoemaker,holiday,week,apple,college,aunt,press,apprentice,pupil,forest\n",
      "Topic 7: tanner,hospital,roof,week,lady,coat,journeyman,tailor,cider,doctor,bottle,slope,fence,beer,tune\n",
      "Topic 8: action,nation,soldier,today,fatherland,mankind,history,solitude,culture,virtue,writer,chapter,shalt,nationalism,camp\n",
      "Topic 9: cloister,abbot,knight,goldmund,mund,beast,instant,plague,scholar,carver,workshop,stall,castle,church,count\n",
      "Topic 10: opera,theatre,violin,tion,piano,conductor,violinist,concert,fint,emotion,singer,week,ition,lool,toboggan\n",
      "Topic 11: tanner,tailor,class,beer,sweetheart,tanning,breaker,religion,church,tramp,razor,marketplace,mayor,roadbook,fence\n",
      "Topic 12: tanner,doctor,razor,hospital,tailor,coffee,sweetheart,tanning,grass,breaker,soup,kitchen,roadbook,today,carriage\n",
      "Topic 13: wolf,dance,aunt,pablo,araucaria,personality,immortal,theater,suicide,bourgeois,dancing,humor,today,professor,dread\n",
      "Topic 14: donation,trademark,term,confirmation,copyright,paragraph,sister,agreement,crest,forehead,holiday,conversation,church,hawk,clergyman\n",
      "Topic 15: carpenter,week,boat,lake,city,neighbor,painter,cripple,saint,tavern,conversation,meadow,tapir,slope,studio\n",
      "Topic 16: studio,painter,doctor,painting,canvas,lake,today,tion,park,carriage,paint,mouse,breakfast,coffee,rosshalde\n",
      "Topic 17: east,archive,document,accuser,official,leader,participant,novitiate,expedition,tent,servant,defendant,manuscript,faith,historian\n",
      "Topic 18: history,elite,player,responsibility,attitude,reader,development,politic,colleague,authority,hierarchy,function,individual,assignment,deputy\n",
      "Topic 19: lake,carpenter,cripple,boat,trip,skiff,farmer,week,peak,sketch,sail,meadow,tain,ment,shore\n"
     ]
    }
   ],
   "source": [
    "#NMF topic model \n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('hesse_file_noun.csv')\n",
    "\n",
    "print('Vectorizing the text data', end=' ', flush=True)\n",
    "vectorizer = TfidfVectorizer(min_df=1, max_df=0.8, stop_words='english',token_pattern=r'\\b\\w{4,}\\b') #remove some scam problem words\n",
    "\"\"\"\n",
    "For small datasets (<100 documents):\n",
    "min_df=1 (removes no words) or min_df=2 (removes words that appear in only one document)\n",
    "max_df=0.8 (removes extremely common words)\n",
    "For medium datasets (100–10,000 documents):\n",
    "min_df=0.01 (1%) → Removes very rare words\n",
    "max_df=0.5 (50%) → Removes overly common words\n",
    "For large datasets (>10,000 documents):\n",
    "min_df=0.02 (2%) or higher\n",
    "max_df=0.4 (40%) to filter frequent terms\n",
    "\"\"\"\n",
    "vectorized_data = vectorizer.fit_transform(df['filtered_text'])\n",
    "print('done', flush=True)\n",
    "\n",
    "print('Fitting the NMF model', end=' ', flush=True)\n",
    "nmf = NMF(n_components=20, random_state=1)\n",
    "doc_topic_distribution_nmf = nmf.fit_transform(vectorized_data)\n",
    "print('done', flush=True)\n",
    "\n",
    "\n",
    "topic_word_df = pd.DataFrame(nmf.components_, columns=vectorizer.get_feature_names_out())\n",
    "for topic, topic_row in topic_word_df.iterrows():\n",
    "    top_15_words = ','.join(topic_row.sort_values(ascending=False).head(15).index)\n",
    "    print(f'Topic {topic}: {top_15_words}')\n",
    "    \n",
    "    \n",
    "#save in csv file\n",
    "df_topic_distribution = pd.DataFrame(doc_topic_distribution_nmf)\n",
    "df_topic_distribution.to_csv(\"document_topic_matrix.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing cosine similarity...\n",
      "\n",
      "Document: beneath_the_wheel.txt - Beneath  The Wheel\n",
      "the_glass_bead_game.txt - The Glass Bead Game: 0.0066\n",
      "demian.txt - Demian: 0.0013\n",
      "peter_camenzind.txt - Peter Camenzind: 0.0000\n",
      "\n",
      "Document: demian.txt - Demian\n",
      "the_glass_bead_game.txt - The Glass Bead Game: 0.0153\n",
      "beneath_the_wheel.txt - Beneath  The Wheel: 0.0013\n",
      "peter_camenzind.txt - Peter Camenzind: 0.0000\n",
      "\n",
      "Document: gertrude.txt - Gertrude\n",
      "the_glass_bead_game.txt - The Glass Bead Game: 0.0000\n",
      "demian.txt - Demian: 0.0000\n",
      "beneath_the_wheel.txt - Beneath  The Wheel: 0.0000\n",
      "\n",
      "Document: if_the_war_goes_on.txt - If The War Goes On\n",
      "the_glass_bead_game.txt - The Glass Bead Game: 0.0000\n",
      "steppenwolf.txt - Steppenwolf: 0.0000\n",
      "rosshalde.txt - Rosshalde: 0.0000\n",
      "\n",
      "Document: in_sight_of_chaos.txt - In  Sight of Chaos\n",
      "the_glass_bead_game.txt - The Glass Bead Game: 0.0000\n",
      "the_journey_to_the_east.txt - The Journey To The East: 0.0000\n",
      "beneath_the_wheel.txt - Beneath  The Wheel: 0.0000\n",
      "\n",
      "Document: knulp.txt - Knulp\n",
      "peter_camenzind.txt - Peter Camenzind: 0.0000\n",
      "the_glass_bead_game.txt - The Glass Bead Game: 0.0000\n",
      "gertrude.txt - Gertrude: 0.0000\n",
      "\n",
      "Document: narziss_and_goldmund.txt - Narziss and Goldmund\n",
      "the_glass_bead_game.txt - The Glass Bead Game: 0.0000\n",
      "siddhartha.txt - Siddhartha: 0.0000\n",
      "demian.txt - Demian: 0.0000\n",
      "\n",
      "Document: peter_camenzind.txt - Peter Camenzind\n",
      "the_glass_bead_game.txt - The Glass Bead Game: 0.0000\n",
      "demian.txt - Demian: 0.0000\n",
      "beneath_the_wheel.txt - Beneath  The Wheel: 0.0000\n",
      "\n",
      "Document: rosshalde.txt - Rosshalde\n",
      "peter_camenzind.txt - Peter Camenzind: 0.0000\n",
      "the_glass_bead_game.txt - The Glass Bead Game: 0.0000\n",
      "knulp.txt - Knulp: 0.0000\n",
      "\n",
      "Document: siddhartha.txt - Siddhartha\n",
      "the_journey_to_the_east.txt - The Journey To The East: 0.0000\n",
      "narziss_and_goldmund.txt - Narziss and Goldmund: 0.0000\n",
      "rosshalde.txt - Rosshalde: 0.0000\n",
      "\n",
      "Document: steppenwolf.txt - Steppenwolf\n",
      "the_glass_bead_game.txt - The Glass Bead Game: 0.0000\n",
      "if_the_war_goes_on.txt - If The War Goes On: 0.0000\n",
      "rosshalde.txt - Rosshalde: 0.0000\n",
      "\n",
      "Document: the_glass_bead_game.txt - The Glass Bead Game\n",
      "demian.txt - Demian: 0.0153\n",
      "beneath_the_wheel.txt - Beneath  The Wheel: 0.0066\n",
      "peter_camenzind.txt - Peter Camenzind: 0.0000\n",
      "\n",
      "Document: the_journey_to_the_east.txt - The Journey To The East\n",
      "the_glass_bead_game.txt - The Glass Bead Game: 0.0000\n",
      "siddhartha.txt - Siddhartha: 0.0000\n",
      "demian.txt - Demian: 0.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'# Save to CSV for further analysis\\nsimilarity_df.to_csv(\"document_similarity_matrix.csv\")\\nprint(\"Cosine similarity analysis complete! Saved as \\'document_similarity_matrix.csv\\'.\")'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "doc_topic_matrix = pd.read_csv(\"document_topic_matrix.csv\")\n",
    "metadata_df = pd.read_csv(\"hesse_file_noun.csv\")\n",
    "metadata = []\n",
    "for i, row in df.iterrows():\n",
    "    text_metadata = f\"{row['filename']} - {row['title']}\"\n",
    "    metadata.append(text_metadata)\n",
    "\n",
    "# Compute cosine similarity matrix\n",
    "print(\"Computing cosine similarity...\")\n",
    "similarity_matrix = cosine_similarity(doc_topic_matrix)\n",
    "\n",
    "# Store similarity in a DataFrame\n",
    "similarity_df = pd.DataFrame(similarity_matrix, index=metadata, columns=metadata)\n",
    "for i, filename in enumerate(metadata):\n",
    "    # Top 3 similar documents\n",
    "    similar_docs = similarity_df.iloc[i].sort_values(ascending=False)[1:4] \n",
    "    print(f\"\\nDocument: {filename}\")\n",
    "    for doc, score in similar_docs.items():\n",
    "        print(f\"{doc}: {score:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_20031",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
